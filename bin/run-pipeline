#!/usr/bin/env python3
import json
import sys
from optparse import OptionParser
import time
import importlib
from multiprocessing import Process

from lettucethink import fsdb

def process_block(block, scan, inputs, outputs):
    block.read_input(scan, inputs)
    block.process()
    block.write_output(scan, outputs)

def find_id(l, id):
    for x in l:
        if x['id'] == id:
            return x
    return None

if __name__ == "__main__":
    usage = "usage: %prog [options] db/scan pipeline"
    parser = OptionParser(usage=usage)

    parser.add_option("-s", "--skip",
        dest="skip",
        default=0,
        type=int,
        help ="skip the first s tasks")

    parser.add_option("-p", "--pipeline",
        dest="pipeline",
        default="",
        type=str,
        help ="skip the first s tasks")

    (options, args) = parser.parse_args()

    if '-n' in sys.argv:
        i = find('-n')

    if len(args) != 1:
        raise Exception('Wrong number of arguments. Type %prog --help for more.')

    scan = args[0]
    scan = scan.split('/')
    db = '/'.join(scan[:-1])
    scan = scan[-1]
    print('DB location = %s'%db)
    print('scan = %s'%scan)

    db = fsdb.DB(db)
    db.connect()

    scan = db.get_scan(scan)

    x = scan.get_fileset("pipeline", create=True)

    if options.pipeline:
        assert(os.path.splitext(options.pipeline)[-1] == ".json")
        f = x.get_file("pipeline", create=True)
        f.import_file(options.pipeline)

    f = x.get_file("pipeline", create=False)
    if f is None:
        raise Exception('No pipeline provided.')

    pipeline = json.loads(f.read_text())
    tasks = []

    remaining_tasks = [ b['id'] for b in pipeline['blocks'] ]
    current_tasks = {}
    finished_tasks = []


    for i,b in enumerate(pipeline['blocks']):
        if i < options.skip:
            continue
        print("Starting task %s (%i/%i)"%(b['id'], i+1, len(pipeline['blocks'])))
        module = importlib.import_module(b['module'])
        block = eval('module.%s'%b['class'])(**b["args"])
        process_block(block, scan, b['inputs'], b['outputs'])
        print("done task %s"%b['id'])

    # while True:
    #     if len(remaining_tasks) == 0 and len(list(current_tasks.keys())) == 0:
    #         break
    #     for k in list(current_tasks.keys()):
    #         # current_tasks[k].join(timeout=0)
    #         if not current_tasks[k].is_alive():
    #             exitcode = current_tasks[k].exitcode
    #             if exitcode != 0:
    #                 raise Exception("Failed task %s"%k)
    #             print("Finished task %s"%k)
    #             finished_tasks.append(k)
    #             current_tasks.pop(k)
    #     if len(current_tasks) < options.nprocs:
    #         for id in remaining_tasks:
    #             b = find_id(pipeline['blocks'], id)
    #             if all(x in finished_tasks for x in b['requires']):
    #                 module = importlib.import_module(b['module'])
    #                 block = eval('module.%s'%b['class'])(**b["args"])
    #                 new_task = Process(target=process_block, args=(block, scan, b['inputs'], b['outputs']))
    #                 new_task.start()
    #                 remaining_tasks.remove(id)
    #                 current_tasks[b['id']] = new_task
    #                 print("Starting task %s"%b['id'])
    #                 break
    #     time.sleep(0.1)



